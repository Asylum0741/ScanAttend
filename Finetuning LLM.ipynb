{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asylum0741/ScanAttend/blob/main/Finetuning%20LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "6jVIiXwmfFHy"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets peft accelerate torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjNkyKRwfNqP"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel, get_peft_model\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the TinyLlama base model and tokenizer\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Move model to GPU (Colab provides free GPUs)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci5lbS_nhSJQ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset from a JSONL file (replace the path with your actual file path)\n",
        "dataset = load_dataset('json', data_files='training_dataset.jsonl', split='train')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2ghDo_inigQ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the raw dataset (Assuming it's in a JSONL format)\n",
        "dataset = load_dataset(\"json\", data_files=\"training_dataset.jsonl\", split=\"train\")\n",
        "\n",
        "# Load tokenizer for TinyLlama\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "\n",
        "# Function to format and tokenize the dataset\n",
        "def format_example(examples):\n",
        "    # Tokenize inputs and outputs\n",
        "    model_inputs = tokenizer(examples['input'], padding=\"max_length\", truncation=True, max_length=512)\n",
        "    labels = tokenizer(examples['output'], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "    # Add the 'labels' key for language model\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "# Apply the formatting function to the dataset\n",
        "tokenized_dataset = dataset.map(format_example, batched=True, num_proc=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZqIwk804NNZ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "f-WpDhq1jtcy"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# Define LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,   # Rank for LoRA\n",
        "    lora_alpha=32,  # Scaling factor for LoRA\n",
        "    lora_dropout=0.1,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Load the TinyLlama model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.gradient_checkpointing_enable()\n",
        "# Apply LoRA to the model\n",
        "lora_model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "lora_model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjxxHu3Kj8Oy"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    fp16=True,  # Use mixed precision training\n",
        "    save_steps=500,\n",
        "    logging_dir=\"./logs\",\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,  # Train dataset\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Start fine-tuning\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5wb-7PY7o7K"
      },
      "outputs": [],
      "source": [
        "# Save the model and tokenizer\n",
        "model.save_pretrained(\"./trained_model\")\n",
        "tokenizer.save_pretrained(\"./trained_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOejE1XE8LyW"
      },
      "outputs": [],
      "source": [
        "!zip -r finetunemodel.zip ./trained_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztLoiLu9EydB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKJZl8DnE57n"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/gdrive', force_remount=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a052XH0FPwQ"
      },
      "outputs": [],
      "source": [
        "!cp ./trained_model/model.safetensors /content/gdrive/MyDrive/\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5TC2UY8S2phW8vC7TGgzk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}